{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll work with neural networks for natural language inference. Our task is: given a premise sentence P and hypothesis H, what entailment relationship holds between them? Is H entailed by P, contradicted by P or neutral towards P?\n",
    "\n",
    "Given a sentence P, if H definitely describe something true given P then it is an **entailment**. If H describe something that's *maybe* true given P, it's **neutral**, and if H describe something that's definitely *false* given P it's a **contradiction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore natural language inference using neural networks on the SNLI dataset, described in [1]. The dataset can be downloaded [here](https://nlp.stanford.edu/projects/snli/). We prepared a \"simplified\" version, with only the relevant columns [here](https://gubox.box.com/s/idd9b9cfbks4dnhznps0gjgbnrzsvfs4).\n",
    "\n",
    "The (simplified) data is organized as follows (tab-separated values):\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "Like in the previous lab, we'll use torchtext to build a dataloader. You can essentially do the same thing as you did in the last lab, but with our new dataset. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path_to_snli, batch_size=8):\n",
    "    \n",
    "    #Fields: premise sent, hypothesis sent, relation label\n",
    "    Tokens = Field(tokenize=lambda x:x.split(), lower=True, batch_first=True) #TODO lowercase?\n",
    "    Labels = Field(batch_first=True)\n",
    "    \n",
    "    fields = [('premise',Tokens),('hypothesis',Tokens),('label',Labels)]\n",
    "    \n",
    "    #Process from csv files\n",
    "    train,test = TabularDataset.splits(\n",
    "            path=path_to_snli, train='simple_snli_1.0_train.csv', test='simple_snli_1.0_test.csv',\n",
    "            format='csv', fields=fields, skip_header=False, \n",
    "            csv_reader_params = {'delimiter':'\\t','quotechar':'„ÄÅ'})\n",
    "    \n",
    "    #Build vocab\n",
    "    Labels.build_vocab(train) # nr of classes\n",
    "    Tokens.build_vocab(train)\n",
    "\n",
    "    #Batch iterator\n",
    "    train_iter, test_iter = BucketIterator.splits(\n",
    "            (train,test), batch_size=batch_size, shuffle=True, device=device,\n",
    "             sort_within_batch=True, sort_key=lambda x: len(x.premise)+len(x.hypothesis))\n",
    "    \n",
    "    return train_iter, test_iter, Tokens.vocab, Labels.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter, vocab, labels = dataloader('simple-snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'entailment', 'contradiction', 'neutral', '-']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "[labels.itos[i] for i in range(len(labels))]  # most golds & predictions should be indexed 2~4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll build the model for predicting the relationship between H and P.\n",
    "\n",
    "We will process each sentence using an LSTM. Then, we will construct some representation of the sentence. When we have a representation for H and P, we will combine them into one vector which we can use to predict the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model described in [2], the BiLSTM with max-pooling model. The procedure for the model is roughly:\n",
    "\n",
    "    1) Encode the Hypothesis and the Premise using one shared bidirectional LSTM (or two different LSTMS)\n",
    "    2) Perform max over the tokens in the premise and the hypothesis\n",
    "    3) Combine the encoded premise and encoded hypothesis into one representation\n",
    "    4) Predict the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "\n",
    "Let's first consider step 2 where we perform max/mean pooling. There is a function in pytorch for this, but we'll implement it from scratch. \n",
    "\n",
    "Let's consider the general case, what we want to do for these methods is apply some function $f$ along dimension $i$, and we want to do this for all $i$'s. As an example we consider the matrix S with size ``(N, D)`` where N is the number of words and D the number of dimensions:\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1d} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{n1} & s_{n2} & s_{n3} & \\dots  & s_{nd}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "What we want to do is apply our function $f$ on each dimension, taking the input $s_{1d}, s_{2d}, ..., s_{nd}$ and generating the output $x_d$. \n",
    "\n",
    "You will implement both the max pooling method. When performing max-pooling, $max$ will be the function which selects a _maximum_ value from a vector and $x$ is the output, thus for each dimension $d$ in our output $x$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_d = max(s_{1d}, s_{2d}, ..., s_{nd})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This operation will reduce a batch of size ``(batch_size, num_words, dimensions)`` to ``(batch_size, dimensions)`` meaning that we now have created a sentence representation based on the content of the words representations in the sentence. \n",
    "\n",
    "Create a function that takes as input a tensor of size ``(batch_size, num_words, dimensions)`` then performs max pooling and returns the result (the output should be of size: ```(batch_size, dimensions)```). [**4 Marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(input_tensor):\n",
    "    indices = torch.argmax(input_tensor, dim=1)\n",
    "    output_tensor = torch.gather(input_tensor, dim=1, index=indices.unsqueeze(dim=1)).squeeze(dim=1)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ should be ``(batch_size, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; |P-H|; P \\cdot H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result.\n",
    "\n",
    "Implement the function. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_premise_and_hypothesis(premise, hypothesis):\n",
    "    \n",
    "    #input: BxD, BxD\n",
    "    p,h = premise, hypothesis\n",
    "    elements = [p, h, abs(p-h), p*h] # BxD, BxD, BxD, BxD\n",
    "\n",
    "    output = torch.cat(elements, dim=1) # Concat vectors in dim1 (skip dim0, ie batchsize)\n",
    "    \n",
    "    return output # Bx(D*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Finally, we can build the model according to the procedure given previously by using the functions we defined above. Additionaly, in the model you should use *dropout*. For efficiency purposes, it's acceptable to only train the model with either max or mean pooling. \n",
    "\n",
    "Implement the model [**6 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, vocab_dim, num_labels, h_dim):\n",
    "        super(SNLIModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_dim, h_dim)\n",
    "        self.rnn = nn.LSTM(h_dim, h_dim, batch_first=True, bidirectional=True )\n",
    "        self.classifier = nn.Linear(8*h_dim, num_labels) \n",
    "        self.dropout = nn.Dropout(0.2) \n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "       #1)Encode the Hypothesis and the Premise using one shared bidirectional LSTM\n",
    "        #Embed premise & hypothesis sentences\n",
    "        p, h = self.embeddings(premise), self.embeddings(hypothesis) #BxNxD\n",
    "        \n",
    "        #Encode through BiLSTM\n",
    "        p,(_, _) = self.rnn(p)# BxNxHD*2\n",
    "        h,(_, _) = self.rnn(h)\n",
    "        p, h = self.dropout(p), self.dropout(h)\n",
    "        \n",
    "       #2)Perform max over the tokens in the premise and the hypothesis\n",
    "        # Max pool the embeddings\n",
    "        # üôÑ we use torch.max here for efficiency reasons cuz the Python way is very slow compared to C++\n",
    "        p_pooled, h_pooled = pooling(p), pooling(h) #BxNxHD*2 => BxHD*2\n",
    "       \n",
    "       #3)Combine the encoded premise and encoded hypothesis into one representation\n",
    "        ph_representation = combine_premise_and_hypothesis(p_pooled, h_pooled) #BxD => B x HD*2*4\n",
    "        \n",
    "       #4)Predict the relationship \n",
    "        predictions = self.classifier(ph_representation)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, implement the training and testing of the model. SNLI can take a very long time to train, so I suggest you only run it for one or two epochs. **[2 marks]** \n",
    "\n",
    "**Tip for efficiency:** *when developing your model, try training and testing the model on one batch (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter, vocab, labels = dataloader('simple-snli', batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training model...\n",
      "Avg loss: 0.7520143614258877\n",
      "Avg loss: 0.5964578376814377\n",
      "Avg loss: 0.5097863134672476\n"
     ]
    }
   ],
   "source": [
    "epochs,learning_rate = 3, 0.001\n",
    "\n",
    "snli_model = SNLIModel( len(vocab), len(labels), 384)\n",
    "snli_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(snli_model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Training model\n",
    "print('Training model...')\n",
    "from statistics import mean\n",
    "total_loss = [] \n",
    "for _ in range(epochs):\n",
    "    \n",
    "    total_loss.clear()\n",
    "    \n",
    "    for i, batch in enumerate(train_iter):\n",
    "        \n",
    "        premise, hypothesis = batch.premise, batch.hypothesis\n",
    "        label = batch.label  # gold labels of batch\n",
    "        \n",
    "        output = snli_model(premise, hypothesis)\n",
    "        loss = loss_function(output, label.view(-1)) # modelout:BxL, target:B\n",
    "        total_loss += [loss.item()]\n",
    "\n",
    "        print(f'Average total loss: {mean(total_loss)}', end='\\r')\n",
    "        \n",
    "        # compute gradients; # update parameters; # reset gradients\n",
    "        loss.backward();     optimizer.step();    optimizer.zero_grad()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average test loss: 0.711665254831314   Tested on : 5440"
     ]
    }
   ],
   "source": [
    "# Test with test_iter\n",
    "test_loss = []\n",
    "snli_model.eval()\n",
    "for i, batch in enumerate(test_iter):\n",
    "    \n",
    "    premise, hypothesis = batch.premise, batch.hypothesis\n",
    "    label = batch.label  # gold labels of batch\n",
    "\n",
    "    with torch.no_grad(): # dont collect gradients when testing\n",
    "        output = snli_model(premise, hypothesis)\n",
    "    batch_loss = loss_function(output, label.view(-1))\n",
    "    test_loss += [batch_loss.item()]\n",
    "\n",
    "    print('Average test loss:', mean(test_loss), '  Tested on :', len(test_loss)*len(batch), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.7369\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Predicted      contradiction  entailment  neutral\n",
       "Golds                                            \n",
       "-                         43          39       94\n",
       "contradiction           2630         119      488\n",
       "entailment               318        2192      858\n",
       "neutral                  464         208     2547"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Predicted</th>\n      <th>contradiction</th>\n      <th>entailment</th>\n      <th>neutral</th>\n    </tr>\n    <tr>\n      <th>Golds</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-</th>\n      <td>43</td>\n      <td>39</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>contradiction</th>\n      <td>2630</td>\n      <td>119</td>\n      <td>488</td>\n    </tr>\n    <tr>\n      <th>entailment</th>\n      <td>318</td>\n      <td>2192</td>\n      <td>858</td>\n    </tr>\n    <tr>\n      <th>neutral</th>\n      <td>464</td>\n      <td>208</td>\n      <td>2547</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "confusion = {'Golds':[], 'Predicted': []}\n",
    "results=[]\n",
    "\n",
    "for i, batch in enumerate(test_iter):\n",
    "    \n",
    "    premise, hypothesis = batch.premise, batch.hypothesis\n",
    "    label = batch.label  # gold labels of batch\n",
    "    \n",
    "    with torch.no_grad(): # dont collect gradients when testing\n",
    "        output = snli_model(premise, hypothesis)\n",
    "        \n",
    "    for b in range( len(batch) ):\n",
    "        \n",
    "        goldlabel = labels.itos[label[b]]\n",
    "        prediction = labels.itos[ torch.argmax(output[b]) ]\n",
    "        \n",
    "        results.append( int(goldlabel==prediction) )\n",
    "        confusion['Golds'].append(goldlabel)\n",
    "        confusion['Predicted'].append(prediction)\n",
    "        \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(confusion, columns=['Golds','Predicted'])\n",
    "matrix = pd.crosstab(df['Golds'], df['Predicted'], rownames=['Golds'], colnames=['Predicted'])\n",
    "print( 'Accuracy:', mean(results) )\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a _baseline_ that we can compare our model against **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple baseline, but a pretty bad one, would be most common class: whichever appears most, contradictions or entailment or neutral, is considered the most likely class, and the baseline is the frequency of that class. E.g. if 50% of the testdata is labeled neutral, 50% would be our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways (other than using a baseline) in which we can analyse the models performance **[4 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could for example evaluate model on another dataset, for example change data to use larger sentences. Another way would be to compare performance with the performance of other simpler models, or pretrained models.\n",
    "\n",
    "We can also compare predictions with human-judged datasets (which would require the our model being trained on the same dataset as the one that the human-judged set is derived from), as well as comparing with other models that were also trained with the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways to improve the model **[3 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model, we max-pooled the premise and hypothesis separately before combining them. Thus some information may be filtered out before the combination. Maybe switching the order of max pooling and combination can let us get the interaction of the two vectors first before extracting/filtering by pooling. Alternatively, we can apply the method mentioned in Talman et al (2019) by feeding the layers both the pooled output from the previous layer as well as the original input, so the information doesn't get too diluted in the deeper layers.\n",
    "\n",
    "Also, Bowman et al (2015) mentioned the importance of syntactic structures of sentences when determining the relationships between premise and hypothesis.\n",
    "\n",
    "We think this aligns with how humans judge the premise vs hypothesis: we would picture/imagine what the sentences describe and decide whether one entails or contradict the other, instead of just comparing the texts themselves (it would be like someone who doesn't understand English but reads Latin script studying our dataset and eventually works out a pattern for classification, despite not understanding the sentence meanings).\n",
    "\n",
    "Therefore, it may be useful to parse the raw sentences into syntax trees first before embedding to help establishing the relations of roles described in a sentence. This is done in the SNLI 1.0 datasets in the sentence_binary_parse and sentence_parse columns. It will be interesting to compare the performance of training with raw sentence vs training with parsed trees.\n",
    "\n",
    "Another way is to use pre-trained model and tokenizer, eg BERT to for the embeddings because interpreting the inference of premise->hypothesis requires world knowledge especially when determining the hierarchical relationships of things (eg, a rabbit is an animal but not vice versa).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings\n",
    "\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). \n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}