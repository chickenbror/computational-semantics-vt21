{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll work with neural networks for natural language inference. Our task is: given a premise sentence P and hypothesis H, what entailment relationship holds between them? Is H entailed by P, contradicted by P or neutral towards P?\n",
    "\n",
    "Given a sentence P, if H definitely describe something true given P then it is an **entailment**. If H describe something that's *maybe* true given P, it's **neutral**, and if H describe something that's definitely *false* given P it's a **contradiction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore natural language inference using neural networks on the SNLI dataset, described in [1]. The dataset can be downloaded [here](https://nlp.stanford.edu/projects/snli/). We prepared a \"simplified\" version, with only the relevant columns [here](https://gubox.box.com/s/idd9b9cfbks4dnhznps0gjgbnrzsvfs4).\n",
    "\n",
    "The (simplified) data is organized as follows (tab-separated values):\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "Like in the previous lab, we'll use torchtext to build a dataloader. You can essentially do the same thing as you did in the last lab, but with our new dataset. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path_to_snli, batch_size=8):\n",
    "    \n",
    "    #Fields: premise sent, hypothesis sent, relation label\n",
    "    Tokens = Field(tokenize=lambda x:x.split(), lower=True, batch_first=True) #TODO lowercase?\n",
    "    Labels = Field(batch_first=True)\n",
    "    \n",
    "    fields = [('premise',Tokens),('hypothesis',Tokens),('label',Labels)]\n",
    "    \n",
    "    #Process from csv files\n",
    "    train,test = TabularDataset.splits(\n",
    "            path=path_to_snli, train='simple_snli_1.0_train.csv', test='simple_snli_1.0_test.csv',\n",
    "            format='csv', fields=fields, skip_header=False, \n",
    "            csv_reader_params = {'delimiter':'\\t','quotechar':'„ÄÅ'})\n",
    "    \n",
    "    #Build vocab\n",
    "    Labels.build_vocab(train) # nr of classes\n",
    "    Tokens.build_vocab(train)\n",
    "\n",
    "    #Batch iterator\n",
    "    train_iter, test_iter = BucketIterator.splits(\n",
    "            (train,test), batch_size=batch_size, shuffle=True, device=device,\n",
    "             sort_within_batch=True, sort_key=lambda x: len(x.premise)+len(x.hypothesis))\n",
    "    \n",
    "    return train_iter, test_iter, Tokens.vocab, Labels.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter, vocab, labels = dataloader('simple-snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'entailment', 'contradiction', 'neutral', '-']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[labels.itos[i] for i in range(len(labels))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll build the model for predicting the relationship between H and P.\n",
    "\n",
    "We will process each sentence using an LSTM. Then, we will construct some representation of the sentence. When we have a representation for H and P, we will combine them into one vector which we can use to predict the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model described in [2], the BiLSTM with max-pooling model. The procedure for the model is roughly:\n",
    "\n",
    "    1) Encode the Hypothesis and the Premise using one shared bidirectional LSTM (or two different LSTMS)\n",
    "    2) Perform max over the tokens in the premise and the hypothesis\n",
    "    3) Combine the encoded premise and encoded hypothesis into one representation\n",
    "    4) Predict the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "\n",
    "Let's first consider step 2 where we perform max/mean pooling. There is a function in pytorch for this, but we'll implement it from scratch. \n",
    "\n",
    "Let's consider the general case, what we want to do for these methods is apply some function $f$ along dimension $i$, and we want to do this for all $i$'s. As an example we consider the matrix S with size ``(N, D)`` where N is the number of words and D the number of dimensions:\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1d} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{n1} & s_{n2} & s_{n3} & \\dots  & s_{nd}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "What we want to do is apply our function $f$ on each dimension, taking the input $s_{1d}, s_{2d}, ..., s_{nd}$ and generating the output $x_d$. \n",
    "\n",
    "You will implement both the max pooling method. When performing max-pooling, $max$ will be the function which selects a _maximum_ value from a vector and $x$ is the output, thus for each dimension $d$ in our output $x$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_d = max(s_{1d}, s_{2d}, ..., s_{nd})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This operation will reduce a batch of size ``(batch_size, num_words, dimensions)`` to ``(batch_size, dimensions)`` meaning that we now have created a sentence representation based on the content of the words representations in the sentence. \n",
    "\n",
    "Create a function that takes as input a tensor of size ``(batch_size, num_words, dimensions)`` then performs max pooling and returns the result (the output should be of size: ```(batch_size, dimensions)```). [**4 Marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 3, 1, 1],\n",
      "         [2, 2, 2, 2],\n",
      "         [3, 3, 3, 3]],\n",
      "\n",
      "        [[1, 3, 1, 1],\n",
      "         [2, 2, 2, 2],\n",
      "         [3, 3, 3, 3]],\n",
      "\n",
      "        [[1, 3, 1, 1],\n",
      "         [2, 2, 2, 2],\n",
      "         [3, 3, 3, 3]],\n",
      "\n",
      "        [[1, 3, 1, 1],\n",
      "         [2, 2, 2, 2],\n",
      "         [3, 3, 3, 3]],\n",
      "\n",
      "        [[1, 3, 1, 1],\n",
      "         [2, 2, 2, 2],\n",
      "         [3, 3, 3, 3]]])\n",
      "tensor([[3, 3, 3, 3],\n",
      "        [3, 3, 3, 3],\n",
      "        [3, 3, 3, 3],\n",
      "        [3, 3, 3, 3],\n",
      "        [3, 3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "def pooling(input_tensor):\n",
    "    \n",
    "    # input tensor: BxNxD \n",
    "    # reduce dim1 to max(N) => Bx1xD\n",
    "    output_tensor, idxs = torch.max( input_tensor, dim=1)\n",
    "    \n",
    "    return output_tensor\n",
    "\n",
    "test_tens = torch.stack( [torch.tensor([[1,3,1,1],[2,2,2,2],[3,3,3,3]])]*5, dim=0)\n",
    "print(test_tens)\n",
    "print( pooling(test_tens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ should be ``(batch_size, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; |P-H|; P \\cdot H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result.\n",
    "\n",
    "Implement the function. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_premise_and_hypothesis(premise, hypothesis):\n",
    "    \n",
    "    #input: BxD, BxD\n",
    "    p,h = premise, hypothesis\n",
    "    \n",
    "    abs_diffs = abs(p-h)\n",
    "    products =  p*h\n",
    "\n",
    "    output = torch.cat([p, h, abs_diffs, products], dim=1) # Concat vector in dim1 (skip dim0, ie batchsize)\n",
    "    \n",
    "    return output # Bx(D*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  3,  3,  3,  4, 10, 18],\n",
       "        [ 1,  2,  3,  4,  5,  6,  3,  3,  3,  4, 10, 18],\n",
       "        [ 1,  2,  3,  4,  5,  6,  3,  3,  3,  4, 10, 18],\n",
       "        [ 1,  2,  3,  4,  5,  6,  3,  3,  3,  4, 10, 18],\n",
       "        [ 1,  2,  3,  4,  5,  6,  3,  3,  3,  4, 10, 18]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = torch.tensor( [[1,2,3]]*5 )\n",
    "two = torch.tensor( [[4,5,6]]*5 )\n",
    "\n",
    "combine_premise_and_hypothesis(one, two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Finally, we can build the model according to the procedure given previously by using the functions we defined above. Additionaly, in the model you should use *dropout*. For efficiency purposes, it's acceptable to only train the model with either max or mean pooling. \n",
    "\n",
    "Implement the model [**6 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, vocab_dim, num_labels, h_dim):\n",
    "        super(SNLIModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_dim, h_dim)\n",
    "        self.rnn = nn.LSTM(h_dim, h_dim, batch_first=True, bidirectional=True )\n",
    "        self.classifier = nn.Linear(8*h_dim, num_labels) \n",
    "        self.dropout = nn.Dropout(0.2) \n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "       #1)Encode the Hypothesis and the Premise using one shared bidirectional LSTM\n",
    "        #Embed premise & hypothesis sentences\n",
    "        p, h = self.embeddings(premise), self.embeddings(hypothesis) #BxNxD\n",
    "        \n",
    "        #Encode through BiLSTM\n",
    "        p,(_, _) = self.rnn(p)# BxNxHD*2\n",
    "        h,(_, _) = self.rnn(h)\n",
    "        p, h = self.dropout(p), self.dropout(h)\n",
    "        \n",
    "       #2)Perform max over the tokens in the premise and the hypothesis\n",
    "        # Max pool the embeddings\n",
    "        p_pooled, h_pooled = pooling(p), pooling(h) #BxNxHD*2 => BxHD*2\n",
    "       \n",
    "       #3)Combine the encoded premise and encoded hypothesis into one representation\n",
    "        ph_representation = combine_premise_and_hypothesis(p_pooled, h_pooled) #BxD => B x HD*2*4\n",
    "        ph_representation = self.dropout(ph_representation)\n",
    "        \n",
    "       #4)Predict the relationship \n",
    "        predictions = self.classifier(ph_representation)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, implement the training and testing of the model. SNLI can take a very long time to train, so I suggest you only run it for one or two epochs. **[2 marks]** \n",
    "\n",
    "**Tip for efficiency:** *when developing your model, try training and testing the model on one batch (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total loss: 0.7616666515482262\r"
     ]
    }
   ],
   "source": [
    "# train_iter, test_iter, vocab, labels = dataloader('simple-snli')\n",
    "\n",
    "epochs,learning_rate = 3, 0.001\n",
    "\n",
    "snli_model = SNLIModel( len(vocab), len(labels), 32)\n",
    "snli_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(snli_model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Training model\n",
    "from statistics import mean\n",
    "total_loss = [] \n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        \n",
    "        premise, hypothesis = batch.premise, batch.hypothesis\n",
    "        label = batch.label  # gold label of batch\n",
    "        \n",
    "        output = snli_model(premise, hypothesis)\n",
    "        \n",
    "#         print(premise.shape) #BxN\n",
    "#         print(output.view(-1,len(labels)).shape) #BxL\n",
    "#         print(label.view(-1).shape) #Bx1\n",
    "#         break\n",
    "        \n",
    "        loss = loss_function(output, label.view(-1)) # modelout:BxL, target:B\n",
    "        total_loss += [loss.item()]\n",
    "\n",
    "        print(f'Average total loss: {mean(total_loss)}', end='\\r')\n",
    "        \n",
    "        # compute gradients; # update parameters; # reset gradients\n",
    "        loss.backward();     optimizer.step();    optimizer.zero_grad()\n",
    "    \n",
    "#     print()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with test_iter\n",
    "test_loss = []\n",
    "snli_model.eval()\n",
    "for i, batch in enumerate(test_iter):\n",
    "    \n",
    "    premise, hypothesis = batch.premise, batch.hypothesis\n",
    "    label = batch.label  # gold labels of batch\n",
    "\n",
    "    with torch.no_grad(): # dont collect gradients when testing\n",
    "        output = snli_model(premise, hypothesis)\n",
    "    batch_loss = loss_function(output, label.view(-1))\n",
    "    test_loss += [batch_loss.item()]\n",
    "\n",
    "    print('Average test loss: ', mean(test_loss), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save:\n",
    "torch.save(snli_model.state_dict(), 'snli_model.pt')\n",
    "\n",
    "# Load:\n",
    "# vocabsize, num_labels, h_dim = len(vocab),len(labels),32  \n",
    "\n",
    "# snli_model = SNLIModel(vocab_dim, num_labels, h_dim)\n",
    "# snli_model.load_state_dict(torch.load('snli_model.pt'), )\n",
    "# snli_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a _baseline_ that we can compare our model against **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways (other than using a baseline) in which we can analyse the models performance **[4 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "Apply the model on the training data to evaluation the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways to improve the model **[3 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings\n",
    "\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). \n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
