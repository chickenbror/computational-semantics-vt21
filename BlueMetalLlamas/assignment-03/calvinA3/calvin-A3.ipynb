{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Word Embeddings and Language Modelling\n",
    "\n",
    "Adam Ek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch. Some basic operations that will be useful can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* In general: we are not interested in getting state-of-the-art performance :) focus on the implementation and not results of your model. For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so, on linux/mac: ```head -n 10000 inputfile > outputfile```. \n",
    "* If possible, use the MLTGpu, it will make everything faster :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE A SMALLER TXT CORPUS\n",
    "# inputfile, outputfile, headn = 'wiki-corpus.txt', 'wiki-subset.txt', 10000\n",
    "# with open(inputfile, encoding='utf8') as f:\n",
    "#     lines_head = [l for l in f][:headn]\n",
    "# with open(outputfile, 'w', encoding='utf8') as f:\n",
    "#     for l in lines_head:\n",
    "#         f.write(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for gpu, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "hardware = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load some data, you can download the file on canvas under files/03-lab-data/wiki-corpus.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the center word in one field and the context words in another (separate the fields with ```tab```).\n",
    "\n",
    "For example, the sentece \"this is a lab\" with ```window size = 4``` will be formatted as:\n",
    "```\n",
    "center, context\n",
    "---------------------\n",
    "this    is a lab\n",
    "is      this a lab\n",
    "a       this is lab\n",
    "lab     this is a\n",
    "```\n",
    "\n",
    "this will be our training examples when training the word2vec model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'wiki-corpus.txt'\n",
    "# data_path = 'wiki-subset.txt'\n",
    "WINDOW_SIZE = 4  # Hyperparameter of context size\n",
    "\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "\n",
    "def corpus_reader(data_path):\n",
    "    with open(data_path, encoding='utf8') as f:\n",
    "      lines = [l.lower().split() for l in f] # tokenized sentences list\n",
    "      center_and_context = []\n",
    "      for l in lines:\n",
    "        l = [token for token in l if token not in punctuation]\n",
    "        \n",
    "#         ngrammize = lambda input_list,n : list(zip(*[input_list[i:] for i in range(n)])) # Turn a tokens list to n-grams list\n",
    "#         ngrams = ngrammize( l, WINDOW_SIZE )\n",
    "#         for gram in ngrams:\n",
    "#           idx_range = range(WINDOW_SIZE)\n",
    "#           for i in idx_range:\n",
    "#             center = gram[i] # One of the words in the ngram\n",
    "#             context = [ gram[j] for j in idx_range if j!=i ] # The rest of the ngram words that isn't the center word\n",
    "#             center_and_context.append(f\"{center}\\t{' '.join(context)}\")\n",
    "\n",
    "        k = int(WINDOW_SIZE/2) # window size to the left or right\n",
    "#         l = ['<START>']*k + l + ['<END>']*k  # start/end tokens to make each context the same length\n",
    "        for i in range( len(l) ):\n",
    "            center, context = l[i], []\n",
    "            context_idx = [i+j for j in range(-k,0)] + [i+j for j in range(1,k+1)] # indexes before and after\n",
    "            for idx in context_idx:\n",
    "                if idx>=0: # If index isn't negative, add as many context (before+after) words as allowed \n",
    "                    try:\n",
    "                        context.append(l[idx])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "#             if center not in ['<START>','<END>']:\n",
    "            if len(context)!=0:\n",
    "                center_and_context.append( [center, f\"{' '.join(context)}\"] )\n",
    "\n",
    "\n",
    "#     # Create new CSV/TSV file where each line is Center<tab>Context\n",
    "    df = pd.DataFrame(center_and_context, columns=['center','context'])\n",
    "    df.to_csv(data_path.replace('.txt','-formatted.csv'), index=False, sep='\\t')\n",
    "\n",
    "\n",
    "corpus_reader(data_path) # Format txt and save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anarchist</td>\n",
       "      <td>historian george</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>historian</td>\n",
       "      <td>anarchist george woodcock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>george</td>\n",
       "      <td>anarchist historian woodcock reports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>woodcock</td>\n",
       "      <td>historian george reports that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reports</td>\n",
       "      <td>george woodcock that the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096648</th>\n",
       "      <td>split</td>\n",
       "      <td>race was into eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096649</th>\n",
       "      <td>into</td>\n",
       "      <td>was split eight stages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096650</th>\n",
       "      <td>eight</td>\n",
       "      <td>split into stages covering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096651</th>\n",
       "      <td>stages</td>\n",
       "      <td>into eight covering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096652</th>\n",
       "      <td>covering</td>\n",
       "      <td>eight stages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096653 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            center                               context\n",
       "0        anarchist                      historian george\n",
       "1        historian             anarchist george woodcock\n",
       "2           george  anarchist historian woodcock reports\n",
       "3         woodcock         historian george reports that\n",
       "4          reports              george woodcock that the\n",
       "...            ...                                   ...\n",
       "1096648      split                   race was into eight\n",
       "1096649       into                was split eight stages\n",
       "1096650      eight            split into stages covering\n",
       "1096651     stages                   into eight covering\n",
       "1096652   covering                          eight stages\n",
       "\n",
       "[1096653 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The saved CSV/TSV looks like this:\n",
    "csvdf = pd.read_csv('wiki-corpus-formatted.csv', sep='\\t')\n",
    "csvdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:** This method embeds each word within a given windows size, which gives us the freedom of adjusting the context size according to the purpose, for example, whether we want the context feature to capture syntactic or semantics information. Also, each token is only represented by one center+context pair so it computes faster during training/model-building. \n",
    "The input from Wikipedia is randomized so it represents a diverse set of data so it suits a wider variety of tasks.\n",
    "(Compare to Note below.)\n",
    "\n",
    "**Cons:**\n",
    "The length of context words varies. Eg if a word is near the start/end of a sentence, its context will lack words before/after and hence carries less information (may be solved by adding start/end tokens on either ends of each sentence). Also, the tokens are derivative forms of words, so a word's different forms are considered different vocab entries; depending on how we want to represent the meanings, it may be a good idea to lemmatize them.\n",
    "Also relating to the random Wikipedia input above, it may not suit tasks that require specific domains, since it's trained on general texts that belong to a bit of everything. Moreover, Wikipedia is crowd-sourced so the content are not necessarily reflecting the truth.\n",
    "\n",
    "**Note:** The original way of formatting (before Adam updated the notebook on May 2; as implemented by the commented lines above) results in N center+context pairs for each token, where N=WINDOW_SIZE. For example, if WINDOW_SIZE=4, then a token is represented by 4 different combinations, namely: \n",
    "\n",
    "    center + [center-3, center-2, center-1]\n",
    "\n",
    "    center + [center-2, center-1, center+1]\n",
    "\n",
    "    center + [center-1, center+1, center+2]\n",
    "\n",
    "    center + [center+1, center+2, center+3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We now need to load the data in an appropriate format for torchtext (https://torchtext.readthedocs.io/en/latest/). We'll use PyText for this and it'll follow the same structure as I showed you in the lecture (remember to lower-case all tokens). Create a function which returns a (bucket)iterator of the training data, and the vocabulary object (```Field```). \n",
    "\n",
    "(*hint1*: you can format the data such that the center word always is first, then you only need to use one field)\n",
    "\n",
    "(*hint2*: the code I showed you during the leture is available in /files/pytorch_tutorial/ on canvas)\n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, TabularDataset, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(datafile):\n",
    "\n",
    "    # \"fields\" that process the columns in TSV files\n",
    "    Tokens = Field(tokenize = lambda x:x.split(), lower=True, batch_first=True)\n",
    "    fields = [('center', Tokens),('context', Tokens)]\n",
    "    \n",
    "    # read the TSV files and create a dataset generator, for example:\n",
    "    #     data[0].center # list of 1 item, eg, ['anarchist']\n",
    "    #     data[4].context # list of context tokens, eg, ['george', 'woodcock', 'that', 'the']\n",
    "    data = TabularDataset(path=datafile, format='csv', fields=fields,\n",
    "                          skip_header=True, csv_reader_params = {'delimiter':'\\t','quotechar':'、'})\n",
    "\n",
    "    # build vocabularies based on what TSV files contained and create word2id mapping\n",
    "    #   len(Center.vocab) == vocabsize\n",
    "    #   Center.vocab[wordstr]==encoding, eg Center.vocab['anarchist']==3334 ; Center.vocab[UnknownWord]==0\n",
    "    Tokens.build_vocab(data, min_freq=1)\n",
    "\n",
    "\n",
    "    # create batches from our data, and shuffle them for each epoch\n",
    "    # TODO Do we need sort_within_batch = True, sort_key= lambda x: len(x.context), \n",
    "    #      since center+context (1+4) are almost same length?\n",
    "    dataset_iter = BucketIterator( data, batch_size= 8, shuffle= True, device= device)\n",
    "\n",
    "    return dataset_iter, Tokens.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, vocab = get_data('wiki-subset-formatted.csv') # From smaller corpus\n",
    "dataset, vocab = get_data('wiki-corpus-formatted.csv') # From whole corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([8, 1]) torch.Size([8])\n",
      "1 torch.Size([8, 1]) torch.Size([8])\n",
      "2 torch.Size([8, 1]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset): # Generated results are random\n",
    "    center = batch.center  \n",
    "    context = batch.context\n",
    "    context = torch.sum(context,dim=1)  # Expects (B,S)=>(B,1)|(B)\n",
    "    print(i,center.shape, context.shape) # Context tensor, shape= 8,S  ie B=batchsize, S=winsize(range 2~4)\n",
    "    \n",
    "    if i==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words have different meanings when they are capitalized vs uncaptalized, so lower-casing them will resulting in them being represented the same way. For exmaple, Turkey (country) and turkey (bird)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim): # Args: vocab_size:int, embed_dim:int\n",
    "        super(CBOWModel, self).__init__()\n",
    "        \n",
    "        #out: 1 x V\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  # Matrix of V*D => 128\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)  # 128 => V\n",
    "\n",
    "    \n",
    "    def forward(self, context):\n",
    "        embedded_context = self.embeddings(context)\n",
    "        embedded_context = self.projection_function(embedded_context) # [B,S,D] => [B,D]\n",
    "        out = self.linear(embedded_context) # nonlinear + projection\n",
    "        log_probs = F.log_softmax(out, dim=1) # softmax log-prob\n",
    "        return log_probs\n",
    "    \n",
    "\n",
    "    def projection_function(self, xs):\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "        xs_sum = torch.sum(xs,dim=1)\n",
    "        return xs_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train the models. First we define which hyperparameters to use. (You can change these, for example when *developing* your model you can use a batch size of 2 and a very low dimensionality (say 10), just to speed things up). When actually training your model *fo real*, you can use a batch size of [8,16,32,64], and embedding dimensionality of [128,256]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs :)\n",
    "word_embeddings_hyperparameters = {'epochs':3,\n",
    "                                   'batch_size':16,\n",
    "                                   'embedding_size':128,\n",
    "                                   'learning_rate':0.001,\n",
    "                                   'embedding_dim':128}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge total loss: 8.0635953482679446\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [39:26<1:18:52, 2366.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averge total loss: 7.9400545899065575\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [1:18:42<39:23, 2363.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge total loss: 7.940056337650774\n",
      "Averge total loss: 7.8440138761463035\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [1:57:44<00:00, 2354.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# dataset, vocab = get_data( 'wiki-corpus-formatted.csv' )\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "cbow_model = CBOWModel(len(vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() #Alt, loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_loss, batch_count = 0, 0 # Keeps accumulating after each epoch; current batch_count is the denominator\n",
    "\n",
    "for epoch in tqdm( range(word_embeddings_hyperparameters['epochs']) ):\n",
    "    epoch_loss = 0 # Reset for every epoch\n",
    "    \n",
    "    for i, batch in enumerate(dataset):\n",
    "        \n",
    "        context = batch.context # tensor of size 8,4 (B,winsize)\n",
    "        center = batch.center # tensor of size 8,1\n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(context)  # output: tensor of size 8,vocsize (B,vocsize)\n",
    "        \n",
    "#         print(context.shape, center.shape, output.shape, sep='\\n'); break  # [8,4], [8,1], [8, 31195]\n",
    "#         print(center.view(-1).shape) ;break  # [8]\n",
    "\n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "        loss = loss_fn(output, center.view(-1))  #in:(B,vocsize), target:(B)\n",
    "        total_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # print average loss for the epoch\n",
    "#         print(f'Averge loss of epoch {epoch+1}: {epoch_loss/(i+1)}', end='\\r')\n",
    "        print(f'Averge total loss: {total_loss/(batch_count+1)}', end='\\r'); batch_count+=1\n",
    "        \n",
    "        # compute gradients; # update parameters; # reset gradients\n",
    "        loss.backward();     optimizer.step();    optimizer.zero_grad()\n",
    "    \n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWModel(\n",
       "  (embeddings): Embedding(80673, 128)\n",
       "  (linear): Linear(in_features=128, out_features=80673, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save:\n",
    "# torch.save(cbow_model.state_dict(), 'cbow_model.pt')\n",
    "\n",
    "# #Load:\n",
    "cbow_model = CBOWModel(len(vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.load_state_dict(torch.load('cbow_model.pt'), )\n",
    "cbow_model.eval()\n",
    "\n",
    "##Save whole model:\n",
    "# torch.save(model, 'cbow_model_whole.pt')\n",
    "\n",
    "# Load whol model:\n",
    "\n",
    "# # Model class must be defined somewhere\n",
    "# # class CBOWModel(nn.Module): ... ^\n",
    "# acbowmodel = torch.load('cbow_model_whole.pt')\n",
    "# acbowmodel.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model on a dataset of word similarities, WordSim353 (http://alfonseca.org/eng/research/wordsim353.html , also avalable in vanvas under files/03-l). The first thing we need to do is read the dataset and translate it to integers. What we'll do is to reuse the ```Field``` that records word indexes (the second output of ```get_data()```) and use it to parse the file.\n",
    "\n",
    "The wordsim data is structured as follows:\n",
    "\n",
    "```\n",
    "word1 word2 score\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The ```Field``` we got from ```read_data()``` has two built-in functions, ```stoi``` which maps a string to an integer and ```itos``` which maps an integer to a string. \n",
    "\n",
    "What our datareader needs to do is: \n",
    "\n",
    "```\n",
    "for line in file:\n",
    "    word1, word2, score = file.split()\n",
    "    # encode word1 and word2 as integers\n",
    "    word1_idx = vocab.vocab.stoi[word1]\n",
    "    word2_idx = vocab.vocab.stoi[word2]\n",
    "```\n",
    "\n",
    "when we have the integers for ```word_1``` and ```word2``` we'll compute the similarity between their word embeddings with *cosine simlarity*. We can obtain the embeddings by querying the embedding layer of the model.\n",
    "\n",
    "We calculate the cosine similarity for each word pair in the dataset, then compute the pearson correlation between the similarities we obtained with the scores given in the dataset. \n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6614"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.itos[3334]  # i to string => anarchist\n",
    "vocab.stoi['anarchist']  # string to i => 3334\n",
    "lookup_tensor = lambda idx,embed : embed( torch.LongTensor([idx]) )  # or torch.LongTensor([idx]).to(device)\n",
    "# cbow_model.embeddings(torch.LongTensor([6301]))\n",
    "lookup_tensor(6301,cbow_model.embeddings) # tensor of the word of the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "def read_wordsim(path, vocab, embeddings):\n",
    "    word_pairs = []\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word1, word2, score = line.split()\n",
    "            word_pairs.append((word1,word2))\n",
    "            \n",
    "            score = float(score)\n",
    "            dataset_sims.append(score)\n",
    "            \n",
    "            # get the index for the word; lower word so idx won't be 0 \n",
    "            word1_idx,word2_idx = vocab.stoi[word1.lower()],vocab.stoi[word2.lower()]\n",
    "            \n",
    "            # get the embedding of the word\n",
    "            lookup_tensor = lambda idx:embeddings(torch.LongTensor([idx])) # or torch.LongTensor([idx]).to(device)\n",
    "            word1_emb,word2_emb = lookup_tensor(word1_idx), lookup_tensor(word2_idx)\n",
    "            \n",
    "            # compute cosine similarity, we'll use the version included in pytorch functional\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.functional.cosine_similarity.html\n",
    "            cosine_similarity = F.cosine_similarity( word1_emb,word2_emb ) # Compares two tensors\n",
    "            \n",
    "            model_sims.append(cosine_similarity.item())\n",
    "    \n",
    "    return dataset_sims, model_sims, word_pairs  # list of golds VS list of embeds cos-sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.16006021]\n",
      " [0.16006021 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "path = 'wordsim_similarity_goldstandard.txt'\n",
    "data, model, pairs = read_wordsim( path, vocab, cbow_model.embeddings ) \n",
    "pearson_correlation = np.corrcoef(data, model)\n",
    "\n",
    "# from scipy import stats \n",
    "# pearson_correlation = stats.pearsonr(data, model)\n",
    "# the non-diagonals give the pearson correlation,\n",
    "print(pearson_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think the performance was bad. A value between 0.1~0.3 means \"positively and weakly correlated\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-10 and worst-10 pairs are computed below. Note that they don't necessarily mean 'most similar vs most dissimilar', but rather pairs whose cosine similarities are closest to / furthest from the WordSim353 scores.\n",
    "\n",
    "The gold scores were judged by humans based on their semantic similarities, while the cosine simliarities from the model are derives from the context they occur in. So even if two words are deemed similar by humans, if they didn't occur in similar contexts in the training data, they don't get high cosine similarities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('tiger', 'cat'): (7.35, 4.44),\n",
       " ('tiger', 'tiger'): (10.0, 10.0),\n",
       " ('plane', 'car'): (5.77, 5.98),\n",
       " ('train', 'car'): (6.31, 5.75),\n",
       " ('television', 'radio'): (6.77, 7.72),\n",
       " ('media', 'radio'): (7.42, 5.35),\n",
       " ('bread', 'butter'): (6.19, 4.95),\n",
       " ('cucumber', 'potato'): (5.92, 4.91),\n",
       " ('doctor', 'nurse'): (7.0, 5.09),\n",
       " ('professor', 'doctor'): (6.62, 5.65),\n",
       " ('student', 'professor'): (6.81, 6.83),\n",
       " ('smart', 'stupid'): (5.81, 5.21),\n",
       " ('wood', 'forest'): (7.73, 5.25),\n",
       " ('money', 'cash'): (9.15, 5.14),\n",
       " ('king', 'queen'): (8.58, 6.77),\n",
       " ('king', 'rook'): (5.92, 4.98),\n",
       " ('bishop', 'rabbi'): (6.69, 5.54),\n",
       " ('fuck', 'sex'): (9.44, 4.84),\n",
       " ('football', 'soccer'): (9.03, 6.17),\n",
       " ('football', 'basketball'): (6.81, 7.24),\n",
       " ('football', 'tennis'): (6.63, 5.36),\n",
       " ('Arafat', 'Jackson'): (2.5, 4.44),\n",
       " ('physics', 'chemistry'): (7.35, 6.46),\n",
       " ('vodka', 'gin'): (8.46, 5.07),\n",
       " ('vodka', 'brandy'): (8.13, 6.01),\n",
       " ('drink', 'eat'): (6.87, 6.22),\n",
       " ('car', 'automobile'): (8.94, 5.37),\n",
       " ('gem', 'jewel'): (8.96, 4.29),\n",
       " ('journey', 'voyage'): (9.29, 5.65),\n",
       " ('boy', 'lad'): (8.83, 5.09),\n",
       " ('coast', 'shore'): (9.1, 7.54),\n",
       " ('asylum', 'madhouse'): (8.87, 5.17),\n",
       " ('magician', 'wizard'): (9.02, 5.08),\n",
       " ('midday', 'noon'): (9.29, 5.42),\n",
       " ('furnace', 'stove'): (8.79, 5.58),\n",
       " ('food', 'fruit'): (7.52, 4.98),\n",
       " ('bird', 'cock'): (7.1, 4.6),\n",
       " ('bird', 'crane'): (7.38, 5.72),\n",
       " ('food', 'rooster'): (4.42, 4.21),\n",
       " ('money', 'dollar'): (8.42, 4.99),\n",
       " ('money', 'currency'): (9.04, 6.09),\n",
       " ('tiger', 'jaguar'): (8.0, 4.22),\n",
       " ('tiger', 'feline'): (8.0, 5.05),\n",
       " ('tiger', 'carnivore'): (7.08, 4.77),\n",
       " ('tiger', 'mammal'): (6.85, 6.07),\n",
       " ('tiger', 'animal'): (7.0, 4.74),\n",
       " ('tiger', 'organism'): (4.77, 5.03),\n",
       " ('tiger', 'fauna'): (5.62, 4.56),\n",
       " ('psychology', 'psychiatry'): (8.08, 4.42),\n",
       " ('psychology', 'science'): (6.71, 6.25),\n",
       " ('psychology', 'discipline'): (5.58, 4.76),\n",
       " ('planet', 'star'): (8.45, 5.19),\n",
       " ('planet', 'moon'): (8.08, 5.89),\n",
       " ('planet', 'sun'): (8.02, 5.99),\n",
       " ('precedent', 'example'): (5.85, 5.58),\n",
       " ('precedent', 'antecedent'): (6.04, 5.22),\n",
       " ('cup', 'tableware'): (6.85, 5.55),\n",
       " ('cup', 'artifact'): (2.92, 5.19),\n",
       " ('cup', 'object'): (3.69, 5.09),\n",
       " ('cup', 'entity'): (2.15, 4.68),\n",
       " ('jaguar', 'cat'): (7.42, 5.37),\n",
       " ('jaguar', 'car'): (7.27, 4.7),\n",
       " ('mile', 'kilometer'): (8.66, 5.76),\n",
       " ('skin', 'eye'): (6.22, 6.6),\n",
       " ('Japanese', 'American'): (6.5, 5.74),\n",
       " ('century', 'year'): (7.59, 5.45),\n",
       " ('announcement', 'news'): (7.56, 4.81),\n",
       " ('doctor', 'personnel'): (5.0, 5.11),\n",
       " ('Harvard', 'Yale'): (8.13, 6.65),\n",
       " ('hospital', 'infrastructure'): (4.63, 5.28),\n",
       " ('life', 'death'): (7.88, 6.62),\n",
       " ('travel', 'activity'): (5.0, 5.76),\n",
       " ('type', 'kind'): (8.97, 6.97),\n",
       " ('street', 'place'): (6.44, 5.23),\n",
       " ('street', 'avenue'): (8.88, 5.09),\n",
       " ('street', 'block'): (6.88, 5.81),\n",
       " ('cell', 'phone'): (7.81, 4.46),\n",
       " ('dividend', 'payment'): (7.63, 5.72),\n",
       " ('calculation', 'computation'): (8.44, 5.31),\n",
       " ('profit', 'loss'): (7.63, 5.22),\n",
       " ('dollar', 'yen'): (7.78, 5.09),\n",
       " ('dollar', 'buck'): (9.22, 5.03),\n",
       " ('phone', 'equipment'): (7.13, 4.95),\n",
       " ('liquid', 'water'): (7.89, 5.23),\n",
       " ('marathon', 'sprint'): (7.47, 4.88),\n",
       " ('seafood', 'food'): (8.34, 5.44),\n",
       " ('seafood', 'lobster'): (8.7, 5.19),\n",
       " ('lobster', 'food'): (7.81, 5.61),\n",
       " ('lobster', 'wine'): (5.7, 5.22),\n",
       " ('championship', 'tournament'): (8.36, 5.84),\n",
       " ('man', 'woman'): (8.3, 7.27),\n",
       " ('man', 'governor'): (5.25, 5.54),\n",
       " ('murder', 'manslaughter'): (8.53, 5.69),\n",
       " ('opera', 'performance'): (6.88, 5.53),\n",
       " ('Mexico', 'Brazil'): (7.44, 6.15),\n",
       " ('glass', 'metal'): (5.56, 5.28),\n",
       " ('aluminum', 'metal'): (7.83, 5.28),\n",
       " ('rock', 'jazz'): (7.59, 6.21),\n",
       " ('museum', 'theater'): (7.19, 5.5),\n",
       " ('shower', 'thunderstorm'): (6.31, 4.81),\n",
       " ('monk', 'oracle'): (5.0, 5.79),\n",
       " ('cup', 'food'): (5.0, 4.96),\n",
       " ('journal', 'association'): (4.97, 6.07),\n",
       " ('street', 'children'): (4.94, 4.73),\n",
       " ('car', 'flight'): (4.94, 5.08),\n",
       " ('space', 'chemistry'): (4.88, 5.51),\n",
       " ('situation', 'conclusion'): (4.81, 7.15),\n",
       " ('word', 'similarity'): (4.75, 5.64),\n",
       " ('peace', 'plan'): (4.75, 5.76),\n",
       " ('consumer', 'energy'): (4.75, 5.8),\n",
       " ('ministry', 'culture'): (4.69, 5.32),\n",
       " ('smart', 'student'): (4.62, 4.97),\n",
       " ('investigation', 'effort'): (4.59, 5.79),\n",
       " ('image', 'surface'): (4.56, 5.33),\n",
       " ('life', 'term'): (4.5, 5.62),\n",
       " ('start', 'match'): (4.47, 6.72),\n",
       " ('computer', 'news'): (4.47, 6.8),\n",
       " ('board', 'recommendation'): (4.47, 6.33),\n",
       " ('lad', 'brother'): (4.46, 5.67),\n",
       " ('observation', 'architecture'): (4.38, 4.65),\n",
       " ('coast', 'hill'): (4.38, 6.33),\n",
       " ('deployment', 'departure'): (4.25, 5.31),\n",
       " ('benchmark', 'index'): (4.25, 5.38),\n",
       " ('attempt', 'peace'): (4.25, 4.74),\n",
       " ('consumer', 'confidence'): (4.13, 5.38),\n",
       " ('start', 'year'): (4.06, 6.12),\n",
       " ('focus', 'life'): (4.06, 5.51),\n",
       " ('development', 'issue'): (3.97, 5.88),\n",
       " ('theater', 'history'): (3.91, 5.51),\n",
       " ('situation', 'isolation'): (3.88, 5.76),\n",
       " ('profit', 'warning'): (3.88, 5.02),\n",
       " ('media', 'trading'): (3.88, 4.71),\n",
       " ('chance', 'credibility'): (3.88, 5.07),\n",
       " ('precedent', 'information'): (3.85, 5.44),\n",
       " ('architecture', 'century'): (3.78, 4.84),\n",
       " ('population', 'development'): (3.75, 6.1),\n",
       " ('stock', 'live'): (3.73, 4.05),\n",
       " ('peace', 'atmosphere'): (3.69, 5.92),\n",
       " ('morality', 'marriage'): (3.69, 5.12),\n",
       " ('minority', 'peace'): (3.69, 4.51),\n",
       " ('atmosphere', 'landscape'): (3.69, 6.25),\n",
       " ('report', 'gain'): (3.63, 5.57),\n",
       " ('music', 'project'): (3.63, 4.79),\n",
       " ('seven', 'series'): (3.56, 5.39),\n",
       " ('experience', 'music'): (3.47, 5.21),\n",
       " ('school', 'center'): (3.44, 5.98),\n",
       " ('five', 'month'): (3.38, 4.9),\n",
       " ('announcement', 'production'): (3.38, 5.96),\n",
       " ('morality', 'importance'): (3.31, 5.68),\n",
       " ('money', 'operation'): (3.31, 6.06),\n",
       " ('delay', 'news'): (3.31, 5.48),\n",
       " ('governor', 'interview'): (3.25, 5.21),\n",
       " ('practice', 'institution'): (3.19, 6.07),\n",
       " ('century', 'nation'): (3.16, 5.15),\n",
       " ('coast', 'forest'): (3.15, 6.3),\n",
       " ('shore', 'woodland'): (3.08, 4.98),\n",
       " ('drink', 'car'): (3.04, 6.14),\n",
       " ('president', 'medal'): (3.0, 6.24),\n",
       " ('prejudice', 'recognition'): (3.0, 5.24),\n",
       " ('viewer', 'serial'): (2.97, 5.59),\n",
       " ('peace', 'insurance'): (2.94, 5.21),\n",
       " ('Mars', 'water'): (2.94, 4.56),\n",
       " ('media', 'gain'): (2.88, 4.88),\n",
       " ('precedent', 'cognition'): (2.81, 5.66),\n",
       " ('announcement', 'effort'): (2.75, 6.4),\n",
       " ('line', 'insurance'): (2.69, 4.78),\n",
       " ('crane', 'implement'): (2.69, 5.38),\n",
       " ('drink', 'mother'): (2.65, 5.55),\n",
       " ('opera', 'industry'): (2.63, 5.89),\n",
       " ('volunteer', 'motto'): (2.56, 5.36),\n",
       " ('listing', 'proximity'): (2.56, 5.25),\n",
       " ('precedent', 'collection'): (2.5, 5.01),\n",
       " ('cup', 'article'): (2.4, 4.83),\n",
       " ('sign', 'recess'): (2.38, 4.74),\n",
       " ('problem', 'airport'): (2.38, 5.33),\n",
       " ('reason', 'hypertension'): (2.31, 5.59),\n",
       " ('direction', 'combination'): (2.25, 7.11),\n",
       " ('Wednesday', 'news'): (2.22, 5.95),\n",
       " ('glass', 'magician'): (2.08, 4.91),\n",
       " ('cemetery', 'woodland'): (2.08, 4.61),\n",
       " ('possibility', 'girl'): (1.94, 5.5),\n",
       " ('cup', 'substance'): (1.92, 5.24),\n",
       " ('forest', 'graveyard'): (1.85, 4.31),\n",
       " ('stock', 'egg'): (1.81, 5.19),\n",
       " ('month', 'hotel'): (1.81, 5.35),\n",
       " ('energy', 'secretary'): (1.81, 5.15),\n",
       " ('precedent', 'group'): (1.77, 5.04),\n",
       " ('production', 'hike'): (1.75, 5.84),\n",
       " ('stock', 'phone'): (1.62, 5.18),\n",
       " ('holy', 'sex'): (1.62, 5.88),\n",
       " ('stock', 'CD'): (1.31, 5.31),\n",
       " ('drink', 'ear'): (1.31, 5.7),\n",
       " ('delay', 'racism'): (1.19, 5.28),\n",
       " ('stock', 'life'): (0.92, 4.86),\n",
       " ('stock', 'jaguar'): (0.92, 5.24),\n",
       " ('monk', 'slave'): (0.92, 6.25),\n",
       " ('lad', 'wizard'): (0.92, 5.14),\n",
       " ('sugar', 'approach'): (0.88, 5.22),\n",
       " ('rooster', 'voyage'): (0.62, 5.48),\n",
       " ('noon', 'string'): (0.54, 5.86),\n",
       " ('chord', 'smile'): (0.54, 5.98),\n",
       " ('professor', 'cucumber'): (0.31, 4.4),\n",
       " ('king', 'cabbage'): (0.23, 4.78)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaleto10 = lambda x, minval, maxval : round( 10*(x-minval)/(maxval-minval),2 )\n",
    "{pair: (gold, scaleto10(cos_sim,-1,1)) for pair,gold,cos_sim in list(zip(pairs, data, model)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize to the scale of the respective min & max in data and model:\n",
      "[('tiger', 'tiger'), ('announcement', 'production'), ('school', 'center'), ('problem', 'airport'), ('drink', 'mother'), ('precedent', 'cognition'), ('precedent', 'group'), ('population', 'development'), ('start', 'match'), ('atmosphere', 'landscape')]\n",
      "[('boy', 'lad'), ('cell', 'phone'), ('street', 'avenue'), ('magician', 'wizard'), ('money', 'cash'), ('psychology', 'psychiatry'), ('dollar', 'buck'), ('tiger', 'jaguar'), ('fuck', 'sex'), ('gem', 'jewel')]\n",
      "\n",
      "Normalize to 0~10 for data and -1~1 for model:\n",
      "[('tiger', 'tiger'), ('student', 'professor'), ('cup', 'food'), ('doctor', 'personnel'), ('car', 'flight'), ('street', 'children'), ('plane', 'car'), ('food', 'rooster'), ('tiger', 'organism'), ('precedent', 'example')]\n",
      "[('sugar', 'approach'), ('drink', 'ear'), ('king', 'cabbage'), ('fuck', 'sex'), ('gem', 'jewel'), ('rooster', 'voyage'), ('direction', 'combination'), ('noon', 'string'), ('monk', 'slave'), ('chord', 'smile')]\n"
     ]
    }
   ],
   "source": [
    "# Normalize to range 0~1, where 0/1 is lowest/highest value of all\n",
    "normalize = lambda x, minval, maxval : (x-minval)/(maxval-minval) \n",
    "# The difference between normalized gold vs normalized cos-sim\n",
    "goldmin,goldmax, cosmin,cosmax = min(data),max(data), min(model),max(model)\n",
    "# Normalize to scale of min~max or absolute min~max\n",
    "deviate_relative = lambda gold, cos_sim : abs( normalize(gold, goldmin,goldmax) - normalize(cos_sim, cosmin,cosmax) ) \n",
    "deviate_absolute = lambda gold, cos_sim : abs( normalize(gold, 0,10) - normalize(cos_sim, -1,1) ) \n",
    "\n",
    "print('Normalize to the scale of the respective min & max in data and model:')\n",
    "pair_diff = { pair: deviate(gold,cos_sim) for pair,gold,cos_sim in list(zip(pairs, data, model)) }\n",
    "best2worst = sorted( pair_diff.items(), key= lambda item:item[1]) # From min difference to max difference\n",
    "best10, worst10 = best2worst[:10], best2worst[-10:]\n",
    "print([p[0] for p in best10])  # word pairs whose cos-sims are closest to gold scores\n",
    "print([p[0] for p in worst10]) # word pairs whose cos-sims deviate the most from gold scores\n",
    "\n",
    "print('\\nNormalize to 0~10 for data and -1~1 for model:')\n",
    "pair_diff = { pair: deviate_absolute(gold,cos_sim) for pair,gold,cos_sim in list(zip(pairs, data, model)) }\n",
    "best2worst = sorted( pair_diff.items(), key= lambda item:item[1]) # From min difference to max difference\n",
    "best10, worst10 = best2worst[:10], best2worst[-10:]\n",
    "print([p[0] for p in best10])  # word pairs whose cos-sims are closest to gold scores\n",
    "print([p[0] for p in worst10]) # word pairs whose cos-sims deviate the most from gold scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use other data form training. As mentioned before, the Wikipedia corpus is too diverse and too general. Or just having one corpus file is not enough and we simply need even larger amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hurt the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros for sentiment analysis:**\n",
    "\n",
    "Since the model regards words in similar contexts as semantically similar, it can give the same sentiment ratings (or classifications) for text with synonymous modifiers, eg, great/awesome/terrific.\n",
    "\n",
    "**Cons for sentiment analysis:**\n",
    "\n",
    "From our evaluation above we saw the model didn't capture word similarities as how humans see them. Also, it cannot  disambiguate words with multiple meaning as well as humans do, therfore may result in the wrong classification in sentiment analysis. For example, there are contronyms which are same words with opposite meanings, eg, `fast: quick vs stuck; sanction: appove vs boycott`. \n",
    "\n",
    "Also it looks like that the Wikipedia corpua mixes a lot of languages, not just English, so a word with the same spelling may have different meanings in another language and appear in different contexts, which kind of 'pollutes' its representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predict the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-cropus.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "For this we'll build a new dataloader with torchtext, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token, there is a keyword argument in ```Field``` for this :). But other than that, as before you read the dataset and output a iterator over the dataset and a vocabulary. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before :)\n",
    "lm_hyperparameters = {'epochs':3,\n",
    "                      'batch_size':16,\n",
    "                      'learning_rate':0.001,\n",
    "                      'embedding_dim':128,\n",
    "                      'output_dim':128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'wiki-corpus.txt'\n",
    "data_path = 'wiki-corpus.txt'\n",
    "\n",
    "def get_data_lm(filename):\n",
    "    # your code here, roughly the same as for the word2vec dataloader\n",
    "    csvfile = filename.replace('.txt', '-lm.csv')\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        lines = [l for l in f]\n",
    "        pd.DataFrame(lines).to_csv(csvfile, index=False)\n",
    "    Sentence = Field(lower=True, tokenize=lambda x:x.split(), init_token='<start>', eos_token='<end>', batch_first=True)\n",
    "    Examples = TabularDataset(path=csvfile, format='csv', fields=[('sentence', Sentence)])\n",
    "    \n",
    "    Sentence.build_vocab(Examples)\n",
    "    \n",
    "    dataset_iter = BucketIterator( Examples, batch_size= 8, shuffle= True, device= device)\n",
    "        \n",
    "    return dataset_iter, Sentence.vocab\n",
    "\n",
    "lm_data, lm_vocab = get_data_lm(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " 'as',\n",
       " 'of',\n",
       " 'the',\n",
       " '2000',\n",
       " 'census',\n",
       " ',',\n",
       " 'the',\n",
       " 'wheeling',\n",
       " ',',\n",
       " 'wv',\n",
       " 'msa',\n",
       " 'had',\n",
       " 'a',\n",
       " 'population',\n",
       " 'of',\n",
       " '153,172',\n",
       " '.',\n",
       " '<end>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lm_vocab.stoi['<end>']\n",
    "nums = [    2,    17,     7,     4,   120,    67,     5,     4, 18812,     5,\n",
    "          79571,  8281,    36,    10,    43,     7, 36790,     6,     3,     1,\n",
    "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "              1]\n",
    "[lm_vocab.itos[n] for n in nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[    2,    17,     7,     4,   120,    67,     5,     4, 18812,     5,\n",
       "          79571,  8281,    36,    10,    43,     7, 36790,     6,     3,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    2,  1492,   174,     7,  1076, 19623,  2555,    66,   362,   102,\n",
       "              4,   269, 31933,     5,     4,  6350,     7,   112,   147,     4,\n",
       "          31932,     5,    33,   521,     4, 18840,    66,     8,   703,  3543,\n",
       "              6,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    2, 11423,    25,    55,   674,    36,    64,   316,     9,  2936,\n",
       "              5,     8,    10,  5321,  7530,    13,  1462,     9,  2262,     6,\n",
       "              3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    2,     4,  7124, 11205,    13,  6317, 64611,     6,     3,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    2,    34,  1605,  1324,     5, 12843,     5,  7109,     8,  8596,\n",
       "           1579, 22622,    11,  3629,     8,     4,  4442,     7,    51,   800,\n",
       "            832,     9,     4,   723,     6,     3,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    2,     4,  1502,  1437,   447,   559,     7,  1322, 56861,    12,\n",
       "             10,  1449,    11,    14,  9310, 53076, 29682,     8,    54, 13012,\n",
       "             14,     9,  1433, 16004,     6,     3,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    2,   591, 10145, 60064,     8,   606,  4776,   989,     4,   430,\n",
       "             21,   791,   238,  4553,    69,   961,  1763,     7,  1577,   188,\n",
       "              6,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    2,     9,   538,  1597,  2182,    23,  2111,     7, 44029,  8714,\n",
       "             36,  4764,    19,   591,    22,     9,     4,    58,    57,     5,\n",
       "              8,    19,    78,    22,  1478,     5,    20,   224,     7,     4,\n",
       "           3173,   147,  2883,    11,     4,  1779,     7, 19944,  8714,     6,\n",
       "              3]], device='cuda:0')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(lm_data)\n",
    "# len(lm_vocab)-len(vocab)\n",
    "[b.sentence for i,b in enumerate(lm_data)][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(self, vocabsize, embed_dim, output_dim, num_layers=1, bidirectional=False):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocabsize-1, embed_dim)\n",
    "        self.LSTM = nn.LSTM(embed_dim, output_dim,\n",
    "                            num_layers=num_layers,bidirectional=bidirectional, batch_first=True\n",
    "                    )\n",
    "        self.predict_word = nn.Linear(output_dim, vocabsize-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        embedded_seq = self.embeddings(seq)\n",
    "        timestep_reprentation, (h_n, c_n) = self.LSTM(embedded_seq)\n",
    "        outputs = self.predict_word(h_n.squeeze(0))\n",
    "        predicted_words = self.sigmoid(outputs)\n",
    "        \n",
    "        return predicted_words\n",
    "    \n",
    "#         out, (h_n, c_n) = self.LSTM(seq, None)\n",
    "#         outputs = self.predict_word(h_n.squeeze(0))\n",
    "\n",
    "#         return self.sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LM_withLSTM(\n",
       "  (embeddings): Embedding(80672, 128)\n",
       "  (LSTM): LSTM(128, 128, batch_first=True)\n",
       "  (predict_word): Linear(in_features=128, out_features=80672, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "# lm_dataset, lm_vocab = get_data('wiki-corpus.txt')\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(len(vocab), \n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['output_dim'])\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lm_optimizer = optim.Adam(cbow_model.parameters(), lr=lm_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "# lm_total_loss, lm_batch_count = 0, 0\n",
    "# for epoch in range(lm_hyperparameters['epochs']):\n",
    "#     for i, batch in enumerate(dataset):\n",
    "        \n",
    "#         # the strucure for each BATCH is:\n",
    "#         # <start>, w0, ..., wn, <end>\n",
    "#         sentence = batch.sentence\n",
    "        \n",
    "#         # when training the model, at each input we predict the *NEXT* token\n",
    "#         # consequently there is nothing to predict when we give the model \n",
    "#         # <end> as input. \n",
    "#         # thus, we do not want to give <end> as input to the model, select \n",
    "#         # from each batch all tokens except the last. \n",
    "#         # tip: use pytorch indexing/slicing (same as numpy) \n",
    "#         # (https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)\n",
    "#         # (https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)\n",
    "#         input_sentence = ...\n",
    "        \n",
    "#         # send your batch of sentences to the model\n",
    "#         output = lm_model(input_sentence)\n",
    "        \n",
    "#         # for each output, the model predict the NEXT token, so we have to reshape \n",
    "#         # our dataset again. On timestep t, we evaluate on token t+1. That is,\n",
    "#         # we never predict the <start> token ;) so this time, we select all but the first \n",
    "#         # token from sentences (that is, all the tokens that we predict)\n",
    "#         gold_data = ...\n",
    "        \n",
    "#         # the shape of the output and sentence variable need to be changed,\n",
    "#         # for the loss function. Details are in the documentation.\n",
    "#         # You can use .view(...,...) to reshape the tensors  \n",
    "#         loss = loss_func(...)\n",
    "#         lm_total_loss += lm_loss.item()\n",
    "        \n",
    "#         # print average loss for the epoch\n",
    "#         print(lm_total_loss/(batch_count+1), end='\\r') \n",
    "        \n",
    "#         # compute gradients\n",
    "#         loss.backward()\n",
    "#         # update parameters\n",
    "#         lm_optimizer.step()\n",
    "#         # reset gradients\n",
    "#         lm_optimizer.zero_grad()\n",
    "        \n",
    "#     print()\n",
    "\n",
    "lm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "import json\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "    \n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "            \n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = ...\n",
    "            tok_bad_s = ...\n",
    "            \n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "            enc_good_s = torch.tensor([_ for x in tok_good_s], device=device).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([_ for x in tok_bad_s], device=device).unsqueeze(0)\n",
    "            \n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            good_s = LM_withLSTM(enc_good_s)\n",
    "            bad_s = LM_withLSTM(enc_bad_s)\n",
    "            \n",
    "            # get probabilities with softmax\n",
    "            gs_probs = F.softmax(...)\n",
    "            bs_probs = F.softmax(...)\n",
    "            \n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "            \n",
    "            accuracy.append(int(gs_sent_prob>bs_sent_prob))\n",
    "            \n",
    "    return accuracy\n",
    "            \n",
    "def find_token_probs(model_probs, encoded_sentece):\n",
    "    probs = []\n",
    "\n",
    "    # iterate over the tokens in your encoded sentence\n",
    "    for token, gold_token in enumerate(encoded_sentece):\n",
    "        # select the probability of the gold tokens and save\n",
    "        # hint: pytorch indexing is helpful here ;)\n",
    "        prob = ...\n",
    "        probs.append(prob)\n",
    "    sentence_prob = ...\n",
    "    return sentence_prob\n",
    "\n",
    "path = 'existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(path, ..., ...)\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "* Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "* T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "* T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total marks: 63"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
